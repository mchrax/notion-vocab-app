from dotenv import load_dotenv
import os
import openai
import requests
import json
import re
from datetime import datetime, timezone
import streamlit as st

# --- .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ ---
load_dotenv()

# --- ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾— ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
DATABASE_ID = os.getenv("NOTION_DATABASE_ID")

# --- OpenAIã®ã‚­ãƒ¼ã‚’ã‚»ãƒƒãƒˆ ---
openai.api_key = OPENAI_API_KEY

# ================== IPA â†’ Stress ç”Ÿæˆï¼ˆè¾æ›¸é¢¨ï¼‰ ==================

VOWEL_IPA = "aeiouÉ‘É’É”Ã¦ÉªÊŠÉ™Éœ"

def _ensure_dots(ipa_core: str) -> str:
    s = ipa_core
    s = re.sub(r"(?<!\.)(?=[ËˆËŒ])", ".", s)
    s = re.sub(rf"([{VOWEL_IPA}Ë]+[^{VOWEL_IPA}ËˆËŒ.]+)(?=[{VOWEL_IPA}])", r"\1.", s)
    s = re.sub(r"\.{2,}", ".", s)
    return s

def _romanize_syllable(s: str) -> str:
    rep = s
    C = [
        ("tÊƒ", "ch"), ("dÊ’", "j"), ("Êƒ", "sh"), ("Ê’", "zh"),
        ("Î¸", "th"), ("Ã°", "dh"), ("Å‹", "ng"),
    ]
    for k, v in C:
        rep = rep.replace(k, v)
    V = [
        ("oÊŠ", "oh"), ("eÉª", "ay"), ("aÉª", "eye"), ("aÊŠ", "ow"), ("É”Éª", "oy"),
        ("iË", "ee"), ("uË", "oo"),
        ("ÉœË", "er"), ("É‘Ë", "ah"), ("É”Ë", "aw"),
        ("Éª", "i"), ("ÊŠ", "u"), ("ÊŒ", "uh"), ("É™", "uh"), ("Ã¦", "a"),
        ("É‘", "ah"), ("É’", "o"), ("É”", "aw"),
    ]
    for k, v in V:
        rep = rep.replace(k, v)
    rep = rep.replace("Ë", "")
    rep = rep.replace("É¡", "g").replace("É«", "l")
    rep = rep.replace("j", "y")
    rep = rep.lower().strip()
    return rep or s

def accent_from_ipa(ipa: str) -> str:  # CHANGED: ãƒ•ãƒ¬ãƒ¼ã‚ºå¯¾å¿œï¼ˆå˜èªã”ã¨ã«å‡¦ç†ã—ã€ã‚¹ãƒšãƒ¼ã‚¹ç¶­æŒï¼‰
    core = ipa.strip().strip("/[] ")
    if not core:
        return ""
    tokens = [t for t in core.split() if t]  # å˜èªå¢ƒç•Œã‚’ç¶­æŒ
    outs = []
    for tok in tokens:
        tok = _ensure_dots(tok)
        parts = []
        for syl in [x for x in tok.split(".") if x]:
            primary = syl.startswith("Ëˆ")
            secondary = syl.startswith("ËŒ")
            bare = syl.lstrip("ËˆËŒ")
            roman = _romanize_syllable(bare)
            parts.append(roman.upper() if (primary or secondary) else roman.lower())
        res = "-".join(parts).replace("Ëˆ", "").replace("ËŒ", "")
        if len(parts) == 1:
            res = res.upper()
        outs.append(res)
    return " ".join(outs)

# ====== ã‚¿ã‚°å®šç¾© ======
ALLOWED_TAGS = {
    "ç¤¾ä¼šå•é¡Œ", "å£èªOK", "æ›¸ãè¨€è‘‰ãƒ»å ±é“", "ãƒ•ã‚©ãƒ¼ãƒãƒ«",
    "å°‚é–€ç”¨èª", "æ³•å¾‹ç”¨èª", "ãƒ“ã‚¸ãƒã‚¹", "Football",
    "åŒ»å­¦", "ç§‘å­¦ãƒ»æŠ€è¡“", "IT", "ã‚¹ãƒãƒ¼ãƒ„", "æ–‡åŒ–ãƒ»èŠ¸è¡“",
    "é£Ÿã¹ç‰©ãƒ»æ–™ç†", "æ­´å²", "æ”¿æ²»", "è‡ªç„¶ãƒ»ç’°å¢ƒ"
}

# ====== å˜èªãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°è‡ªå‹•åˆ¤å®šï¼ˆGPTãŒç©ºã®æ™‚ã®ä¿é™ºï¼‰ ======
def heuristic_tags(word: str) -> set:
    w = word.lower()
    tags = set()
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
    if w in {"democracy","feminism","inequality","racism","poverty","refugee",
             "gender","discrimination","immigration","homelessness","opioid",
             "climate","activism"}:
        tags.add("ç¤¾ä¼šå•é¡Œ")
    if w in {"lawsuit","litigation","plaintiff","defendant","statute","ordinance",
             "subpoena","appeal","jurisdiction","precedent","constitution",
             "tort","contract"} or w.endswith("act"):
        tags.add("æ³•å¾‹ç”¨èª")
    if w in {"revenue","profit","margin","kpi","roi","stakeholder","synergy",
             "merger","acquisition","quarterly","fiscal","okr","pipeline",
             "invoice","cashflow","ebitda","churn","retention"}:
        tags.add("ãƒ“ã‚¸ãƒã‚¹")
    if w in {"goal","assist","midfielder","striker","forward","defender","winger",
             "offside","penalty","header","fixture","derby","counterattack","pressing"}:
        tags.add("Football")
    if w in {"algorithm","protocol","quantum","neural","latency","throughput",
             "container","orchestration","kubernetes","syntax","blockchain"}:
        tags.add("å°‚é–€ç”¨èª")
    # åŒ»å­¦
    if any(k in w for k in ["doctor","medicine","health","disease","virus","vaccine","hospital","clinic"]):
        tags.add("åŒ»å­¦")
    # ç§‘å­¦ãƒ»æŠ€è¡“
    if any(k in w for k in ["physics","chemistry","biology","experiment","science","scientific","technology","engineering"]):
        tags.add("ç§‘å­¦ãƒ»æŠ€è¡“")
    # IT
    if any(k in w for k in ["computer","algorithm","program","coding","software","hardware","server","database","network","internet"]):
        tags.add("IT")
    # ã‚¹ãƒãƒ¼ãƒ„
    if any(k in w for k in ["baseball","basketball","tennis","cricket","golf","athletic","athlete","sports"]):
        tags.add("ã‚¹ãƒãƒ¼ãƒ„")
    # æ–‡åŒ–ãƒ»èŠ¸è¡“
    if any(k in w for k in ["music","art","painting","film","movie","literature","theater","novel","artist","culture","dance"]):
        tags.add("æ–‡åŒ–ãƒ»èŠ¸è¡“")
    # é£Ÿã¹ç‰©ãƒ»æ–™ç†
    if any(k in w for k in ["food","meal","cuisine","recipe","chef","restaurant","dish","ingredient","cook"]):
        tags.add("é£Ÿã¹ç‰©ãƒ»æ–™ç†")
    # æ­´å²
    if any(k in w for k in ["history","historical","ancient","empire","dynasty","revolution","historian"]):
        tags.add("æ­´å²")
    # æ”¿æ²»
    if any(k in w for k in ["politic","politics","government","election","policy","democracy","diplomacy"]):
        tags.add("æ”¿æ²»")
    # è‡ªç„¶ãƒ»ç’°å¢ƒ
    if any(k in w for k in ["nature","natural","environment","ecology","climate","forest","wildlife","plant","animal"]):
        tags.add("è‡ªç„¶ãƒ»ç’°å¢ƒ")
    # ãƒ¬ã‚¸ã‚¹ã‚¿ï¼ˆå£èªï¼ãƒ•ã‚©ãƒ¼ãƒãƒ«ï¼æ›¸ãè¨€è‘‰ï¼‰
    colloquial = {"hi","yeah","okay","ok","gonna","wanna","dude","bro","buddy",
                  "cool","kinda","sorta","ain't","y'all"}
    formal_keywords = {"therefore","hence","pursuant","notwithstanding","hereby",
                       "whereas","aforementioned","heretofore","therein","thereof"}
    news_keywords = {"summit","ceasefire","sanction","parliament","minister","administration",
                     "diplomacy","alliance","spokesperson","cease-fire"}
    if any(k == w for k in colloquial):
        tags.add("å£èªOK")
    elif any(k == w for k in formal_keywords):
        tags.add("ãƒ•ã‚©ãƒ¼ãƒãƒ«")
    elif any(k == w for k in news_keywords):
        tags.add("æ›¸ãè¨€è‘‰ãƒ»å ±é“")
    # ã‚¿ã‚°é¸æŠï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³å„ªå…ˆã€æ¬¡ã«ãƒ¬ã‚¸ã‚¹ã‚¿ï¼‰
    PRIORITY = [
        "æ³•å¾‹ç”¨èª","ãƒ“ã‚¸ãƒã‚¹","å°‚é–€ç”¨èª","Football",
        "åŒ»å­¦","ç§‘å­¦ãƒ»æŠ€è¡“","IT","ã‚¹ãƒãƒ¼ãƒ„","æ–‡åŒ–ãƒ»èŠ¸è¡“",
        "é£Ÿã¹ç‰©ãƒ»æ–™ç†","æ­´å²","æ”¿æ²»","è‡ªç„¶ãƒ»ç’°å¢ƒ",
        "ç¤¾ä¼šå•é¡Œ",
        "ãƒ•ã‚©ãƒ¼ãƒãƒ«","æ›¸ãè¨€è‘‰ãƒ»å ±é“","å£èªOK"
    ]
    domain_tags = {"æ³•å¾‹ç”¨èª","ãƒ“ã‚¸ãƒã‚¹","å°‚é–€ç”¨èª","Football",
                   "åŒ»å­¦","ç§‘å­¦ãƒ»æŠ€è¡“","IT","ã‚¹ãƒãƒ¼ãƒ„","æ–‡åŒ–ãƒ»èŠ¸è¡“",
                   "é£Ÿã¹ç‰©ãƒ»æ–™ç†","æ­´å²","æ”¿æ²»","è‡ªç„¶ãƒ»ç’°å¢ƒ","ç¤¾ä¼šå•é¡Œ"}
    register_tags = {"å£èªOK","ãƒ•ã‚©ãƒ¼ãƒãƒ«","æ›¸ãè¨€è‘‰ãƒ»å ±é“"}
    domain = [t for t in tags if t in domain_tags]
    register = [t for t in tags if t in register_tags]
    picked = []
    if domain:
        picked.append(sorted(domain, key=lambda x: PRIORITY.index(x))[0])
    if register:
        picked.append(sorted(register, key=lambda x: PRIORITY.index(x))[0])
    if len(picked) < 2:
        for t in sorted(tags, key=lambda x: PRIORITY.index(x)):
            if t not in picked:
                picked.append(t)
                if len(picked) == 2:
                    break
    return set(picked)

# ====== ã“ã“ã‹ã‚‰ï¼šãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œå‡ºï¼†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚’è¿½åŠ  ======
def is_phrase(term: str) -> bool:  # ADDED
    """ç©ºç™½ã‚„ãƒã‚¤ãƒ•ãƒ³ã‚’å«ã‚€ã¨ãƒ•ãƒ¬ãƒ¼ã‚ºæ‰±ã„"""
    return bool(re.search(r"[\s\-]", term.strip()))

def build_prompt(word: str, strict_idiom: bool = False) -> str:  # ADDED
    base = f"""
You are a lexicographer and register expert. Provide the following for '{word}'.

1) Part of Speech (choose exactly one): Noun | Verb | Adjective | Adverb | Preposition | Phrase
2) Definition in Japanese (accurate, concise)
3) A simple example sentence in **English only**
4) IPA with syllable dots and stress marks (Ëˆ primary, ËŒ secondary), *Cambridge style*. Example: ËŒpÉ‘Ër.lÉ™Ëˆmen.tri
5) Katakana (Japanese reading)
6) Tags: choose ANY from this fixed set only:
   ç¤¾ä¼šå•é¡Œ, å£èªOK, æ›¸ãè¨€è‘‰ãƒ»å ±é“, ãƒ•ã‚©ãƒ¼ãƒãƒ«,
   å°‚é–€ç”¨èª, æ³•å¾‹ç”¨èª, ãƒ“ã‚¸ãƒã‚¹, Football,
   åŒ»å­¦, ç§‘å­¦ãƒ»æŠ€è¡“, IT, ã‚¹ãƒãƒ¼ãƒ„,
   æ–‡åŒ–ãƒ»èŠ¸è¡“, é£Ÿã¹ç‰©ãƒ»æ–™ç†, æ­´å², æ”¿æ²», è‡ªç„¶ãƒ»ç’°å¢ƒ
   - Choose up to 2 tags: ideally 1 register tag (å£èªOK / æ›¸ãè¨€è‘‰ãƒ»å ±é“ / ãƒ•ã‚©ãƒ¼ãƒãƒ«) and 1 domain tag.

Return output exactly in the format below (no extra punctuation, no brackets):

Part of Speech: <one of the six>
Definition (JP): <text>
Example Sentence: <English only>
IPA: <IPA with dots and Ëˆ/ËŒ>
Katakana: <ã‚«ã‚¿ã‚«ãƒŠ>
Tags: <comma-separated (<=2) from the allowed set or empty>
""".strip()
    if is_phrase(word) or strict_idiom:
        extra = """
IMPORTANT:
- This looks like a MULTI-WORD EXPRESSION (idiom / set phrase / phrasal or fixed expression).
- Prefer idiomatic or set-phrase meanings over literal word-by-word translation.
- If a domain-specific idiom exists (e.g., football/business/news), output THAT sense and select an appropriate domain tag.
- Do NOT output literal meanings when idiomatic use is common.
""".strip()
        return base + "\n\n" + extra
    return base

# ====== Notion é‡è¤‡ãƒã‚§ãƒƒã‚¯ & æ›´æ–°ãƒ˜ãƒ«ãƒ‘ãƒ¼ ======
def db_has_property(prop_name: str) -> bool:
    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}"
    headers = {
        "Authorization": f"Bearer {NOTION_API_KEY}",
        "Notion-Version": "2022-06-28",
    }
    r = requests.get(url, headers=headers)
    if r.status_code != 200:
        # å–ã‚Œãªã‹ã£ãŸã‚‰å®‰å…¨å´ã§ False
        return False
    props = r.json().get("properties", {})
    return prop_name in props

def find_existing_page_by_word(word: str) -> str | None:
    """DBå†…ã® 'Word' ã‚¿ã‚¤ãƒˆãƒ«ãŒå®Œå…¨ä¸€è‡´ã™ã‚‹æ—¢å­˜ãƒšãƒ¼ã‚¸IDã‚’è¿”ã™ï¼ˆãªã‘ã‚Œã° Noneï¼‰ã€‚"""
    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    headers = {
        "Authorization": f"Bearer {NOTION_API_KEY}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }
    payload = {
        "filter": {
            "property": "Word",
            "title": {"equals": word}
        },
        "page_size": 1
    }
    r = requests.post(url, headers=headers, data=json.dumps(payload))
    if r.status_code != 200:
        print(f"âš ï¸ Notionæ¤œç´¢å¤±æ•—: {r.status_code} {r.text}")
        return None
    results = r.json().get("results", [])
    return results[0]["id"] if results else None

def update_page_properties(page_id: str, properties: dict) -> requests.Response:
    """æ—¢å­˜ãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã ã‘ã‚’æ›´æ–°ã€‚"""
    url = f"https://api.notion.com/v1/pages/{page_id}"
    headers = {
        "Authorization": f"Bearer {NOTION_API_KEY}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }
    payload = {"properties": properties}
    return requests.patch(url, headers=headers, data=json.dumps(payload))

# ====== ç©ºæ›´æ–°é˜²æ­¢ãƒ˜ãƒ«ãƒ‘ãƒ¼ ======
def safe_property_add(props, key, value, is_title=False, is_multi=False):
    """å€¤ãŒç©ºã§ãªã„ã¨ãã ã‘ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’è¿½åŠ """
    if not value:
        return
    if is_title:
        props[key] = {"title": [{"text": {"content": value}}]}
    elif is_multi:
        props[key] = {"multi_select": [{"name": v} for v in sorted(value)]}
    else:
        props[key] = {"rich_text": [{"text": {"content": value}}]}

# ================== ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— ==================
while True:
    word = input("ğŸ“Œ è¿½åŠ ã—ãŸã„å˜èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›ï¼‰ï¼š ").strip()
    if word.lower() == "exit":
        print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
        break
    if not word:
        continue

    norm = re.sub(r"\bbring\s+.+?\s+to the table\b", "bring something to the table", word.strip(), flags=re.I)
    word = norm

    # CHANGED: å›ºå®šã®promptæ–‡å­—åˆ—ã‚’ã‚„ã‚ã€ãƒ•ãƒ¬ãƒ¼ã‚ºæ™‚ã¯ç›´è¨³ç¦æ­¢ã‚’å¼·èª¿
    prompt = build_prompt(word)  # CHANGED

    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=280,
            temperature=0
        )
        output_text = response.choices[0].message.content
    except Exception as e:
        print(f"âŒ OpenAI API error: {e}")
        continue

    lines = [ln.strip() for ln in output_text.split("\n") if ln.strip()]

    def pick(prefix, default=""):
        for ln in lines:
            if ln.startswith(prefix):
                return ln.replace(prefix, "").strip()
        return default

    # CHANGED: ãƒ•ãƒ¬ãƒ¼ã‚ºãªã‚‰æ—¢å®šPOSã‚’ Phrase ã«å¯„ã›ã‚‹
    pos_raw = pick("Part of Speech:", "Phrase" if is_phrase(word) else "Noun")  # CHANGED
    definition_jp = pick("Definition (JP):", "")
    example_sentence = pick("Example Sentence:", "")
    ipa = pick("IPA:", "")
    katakana = pick("Katakana:", "")
    tags_raw = pick("Tags:", "")

    # CHANGED: ç©ºç™½ã¯æ¶ˆã•ãªã„ï¼ˆå˜èªå¢ƒç•Œã‚’ä¿ã¤ãŸã‚ï¼‰
    ipa = ipa.strip("[]/ ")  # CHANGEDï¼ˆ.replace(" ", "") ã‚’å‰Šé™¤ï¼‰
    pron_stress = accent_from_ipa(ipa)

    gpt_tags = {t.strip() for t in tags_raw.split(",") if t.strip()} & ALLOWED_TAGS
    if not gpt_tags:
        gpt_tags = heuristic_tags(word)

    pos_mapping = {
        "Noun": "Noun",
        "Verb": "V[I/T]",
        "Adjective": "Adj.",
        "Adverb": "Adv.",
        "Preposition": "Prep.",
        "Phrase": "Phr."
    }
    # CHANGED: æœªçŸ¥POSæ™‚ã‚‚ãƒ•ãƒ¬ãƒ¼ã‚ºãªã‚‰æœ€çµ‚çš„ã« Phr. ã«å¯„ã›ã‚‹
    pos = pos_mapping.get(pos_raw, "Phr." if is_phrase(word) else "Noun")  # CHANGED
    
    props = {}
    safe_property_add(props, "Word", word, is_title=True)
    props["A Part of Speech"] = {"multi_select": [{"name": pos}]}  # å“è©ã¯å¿…é ˆ
    
    safe_property_add(props, "Definition (JP)", definition_jp)
    safe_property_add(props, "Example Sentence", example_sentence)
    safe_property_add(props, "Stress", pron_stress)
    safe_property_add(props, "IPA", ipa)
    safe_property_add(props, "Katakana", katakana)
    safe_property_add(props, "Tags", gpt_tags, is_multi=True)
    
    if db_has_property("Last Updated"):
        props["Last Updated"] = {"date": {"start": datetime.now(timezone.utc).isoformat()}}
    
    notion_data = {
        "parent": {"database_id": DATABASE_ID},
        "properties": props
    }

    notion_url = "https://api.notion.com/v1/pages"
    notion_headers = {
        "Authorization": f"Bearer {NOTION_API_KEY}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    # ====== é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼šåŒã˜ 'Word' ãŒæ—¢ã«ã‚ã‚Œã°æ–°è¦ä½œæˆã›ãšæ›´æ–° ======
    try:
        existing_id = find_existing_page_by_word(word)
        if existing_id:
            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°
            upd_res = update_page_properties(existing_id, notion_data["properties"])
            if upd_res.status_code in (200, 201):
                print(f"ğŸ” Notionã®ã€{word}ã€ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼")
            else:
                print(f"âŒ Notionæ›´æ–°å¤±æ•—: {upd_res.status_code} {upd_res.text}")
        else:
            # æ–°è¦ä½œæˆ
            crt_res = requests.post(
                "https://api.notion.com/v1/pages",
                headers={
                    "Authorization": f"Bearer {NOTION_API_KEY}",
                    "Content-Type": "application/json",
                    "Notion-Version": "2022-06-28"
                },
                data=json.dumps(notion_data)
            )
            if crt_res.status_code in (200, 201):
                print(f"âœ… Notionã«ã€{word}ã€ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸï¼ğŸ‰")
            else:
                print(f"âŒ Notionã¸ã®è¿½åŠ ã«å¤±æ•—: {crt_res.status_code} {crt_res.text}")

        # ===== å…±é€šã®å‡ºåŠ›å‡¦ç† =====
        print(f"ğŸ“– å“è©: {pos}")
        if definition_jp:
            print(f"ğŸ“œ æ—¥æœ¬èªã®æ„å‘³: {definition_jp}")
        if example_sentence:
            print(f"ğŸ“ ä¾‹æ–‡: {example_sentence}")
        if pron_stress:
            print(f"ğŸ”Š ç™ºéŸ³: {pron_stress}")
        if ipa:
            print(f"ğŸ¯ IPA: {ipa}")
        if katakana:
            print(f"ğŸˆº ã‚«ã‚¿ã‚«ãƒŠ: {katakana}")
        if gpt_tags:
            print(f"ğŸ·ï¸ ã‚¿ã‚°: {', '.join(sorted(gpt_tags))}")
        print()

    except Exception as e:
        print(f"âŒ Notion error: {e}")
